<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="VL-SurgPT: Bridging Vision and Language for Surgical Point Tracking">
  <meta name="keywords" content="VL-SurgPT">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VL-SurgPT: Bridging Vision and Language for Surgical Point Tracking</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://deepmind.com">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://deepmind-tapir.github.io/">
              TAPIR
            </a>
            <a class="navbar-item" href="https://robotap.github.io/">
              RoboTAP
            </a>
            <a class="navbar-item" href="https://deepmind-tapir.github.io/blogpost.html">
              TAPIR Blog Post
            </a>
            <a class="navbar-item" href="https://bootstap.github.io/">
              BootsTAP
            </a>
            <a class="navbar-item" href="https://tapvid3d.github.io/">
              TAPVid-3D
            </a>
            <a class="navbar-item" href="https://tap-next.github.io/">
              TAPNext
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav> -->


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h2 class="title is-2 publication-title">VL-SurgPT: Bridging Vision and Language for Robust Surgical Point Tracking</h2>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                Rulin Zhou<sup>1, 3, 4</sup>,
              </span>
              <span class="author-block">
                Wenglong He<sup>1</sup>,
              </span>
              <span class="author-block">
                An Wang<sup>2, 3</sup>,
              </span>
              <span class="author-block">
                Xuanhui Zeng<sup>1</sup>,
              </span>
              <span class="author-block">
                Jianhang Zhang<sup>1</sup>,
              </span>
              <span class="author-block">
                Xi Zhang<sup>1</sup>,
              </span>
              <span class="author-block">
                Chaowei Zhu<sup>4</sup>,
              </span>
              <span class="author-block">
                Haijun Hu<sup>4</sup>,
              </span>
              <span class="author-block">
                Hongliang Ren<sup>2, 3</sup>,
              </span>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Shenzhen University</span>
              <span class="author-block"><sup>2</sup>Department of Electronic Engineering, The Chinese University of Hong Kong</span>
              <span class="author-block"><sup>3</sup>The Chinese University of Hong Kong Shenzhen Research Institute</span>
              <span class="author-block"><sup>4</sup>Shenzhen People’s Hospital</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://github.com/SzuPc/VL-SurgPT-Dataset"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Data & Code</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <div class="publication-video" style="padding-bottom: 55%">
              <video id="teaser" autoplay controls muted loop playsinline height="100%">
                <source src="https://storage.googleapis.com/dm-tapnet/tap_vid_zoom_v9.mp4" type="video/mp4">
              </video>
            </div>
            <p>
              Visualization of TAP-Vid Dataset with Human Annotated Ground-Truth Point Tracks
            </p>
          </div>
        </div>
      </div>
    </div>
  </section> -->

  <section class="section">
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h3 class="title is-3">Abstract</h3>
              <!-- 插入PDF图片，宽度100% -->
              <div style="width:100%;margin-bottom:24px;">
                <!-- 将 PDF embed 替换为图片展示 -->
                <img src="static/images/introduction.jpg" alt="Visualization Figure" style="width:100%;display:block;margin-bottom:24px;"/>
              </div>
              <div class="content has-text-justified">
                <p>
                  Accurate point tracking in surgical environments remains challenging due to complex visual conditions including smoke occlusion, specular reflections, and tissue deformation. Current surgical tracking datasets provide only visual coordinates, lacking semantic descriptions of why tracking fails under specific conditions. We present VL-SurgPT, a comprehensive multimodal dataset that bridges visual tracking with textual descriptions of point status.
                  The dataset comprises 908 in vivo video clips, including 754 for tissue tracking (17,171 annotated points across five challenging scenarios) and 154 for instrument tracking (covering seven instrument types with detailed keypoint annotations). We establish comprehensive benchmarks using eight state-of-the-art trackers and propose TG-SurgPT, a text-guided tracking method that leverages semantic descriptions to improve robustness.
                  Experimental results demonstrate significant improvements in tracking accuracy, particularly under adverse visual conditions where traditional methods struggle. This work represents the first large-scale vision-language dataset for surgical scene tracking, enabling development of semantically-aware tracking systems for computer-assisted surgery. Further details and visualizations are available at: <a href="https://szupc.github.io/VL_SurgPT/" target="_blank">https://szupc.github.io/VL_SurgPT/</a>.
                </p>
              </div>
            </div>
          </div>

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h3 class="title is-3">Dataset Introduction</h3>
          <img src="static/images/dataset.jpg" alt="Dataset Visualization" style="width:100%;display:block;margin-bottom:16px;"/>
          <div class="content has-text-justified" style="font-size:1rem;">
            Data collection and annotation workflow for VL-SurgPT. (A) In vivo surgical setup using the da Vinci Xi system.
            (B) Ground truth acquisition using Indocyanine Green (ICG) fluorescent markers under UV illumination. (C-D) Annotation
            interface for point tracking and semantic labeling at 1 fps. (E) Coverage of 7 types of surgical instruments, 9 distinct visual
            status descriptions, and 5 representative challenging scenarios across our dataset.
          </div>
          <img src="static/images/dataset_compare.jpg" alt="Dataset Visualization" style="width:100%;display:block;margin-bottom:16px;"/>
          <div class="content has-text-justified" style="font-size:1rem;">
            We compared the point tracking datasets on tissue: SuPer, Sem.SuPer, SurgT, STIR, SugrgMotion (Tissue); the key point datasets of surgical instruments: EndoVis15, Du et al., SurgPose, SurgMotion (Instrument).
          </div>
        </div>
      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h3 class="title is-3" id="annotation-workflow">Annotation Workflow</h3>
          <div class="publication-video" style="padding-bottom: 50%">
            <iframe src="https://www.youtube.com/embed/X26YzaCDNL0?autoplay=1&mute=1&loop=1&playlist=X26YzaCDNL0" width="640" height="480" allow="autoplay" style="width:100%;"></iframe>
          </div>
          <div class="content has-text-justified">
            <p>
              The video above shows how we work with doctors to annotate 1fps videos. We use PYQT5 to create software that can be used to annotate points on instruments and tissues in surgical scenes. On tissues, we mainly annotate at the junctions of blood vessels (where the visual effect is obvious). On instruments, we need to annotate key points, including the tip of the instrument, joints, main shaft, and 3-7 key points.
            </p>
          </div>
        </div>
      </div>


      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h3 class="title is-3"></h3>
          <div class="container">
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item">
                <iframe width="100%" height="240" src="https://www.youtube.com/embed/I7CMCLqgmEU?autoplay=1&mute=1&loop=1&playlist=I7CMCLqgmEU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              </div>
            </div>
          </div>
        </div>
      </div>        

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h3 class="title is-3">VL-SurgPT Dataset Visualization</h3>
          <div class="container">
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item">
                <iframe width="100%" height="480" src="https://www.youtube.com/embed/I7CMCLqgmEU?autoplay=1&mute=1&loop=1&playlist=I7CMCLqgmEU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              </div>
            </div>
            <div style="font-size:0.95rem;color:#888;margin-top:8px;">
              Shows the location and status of our annotation points in 5 different surgical scenes and on 7 different instruments
            </div>
          </div>
        </div>
      </div>



      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h3 class="title is-3" id="TG-SurgPT">Method Details</h3>
          <img src="static/images/Method_details.jpg" alt="Dataset Visualization" style="width:100%;display:block;margin-bottom:16px;"/>
          <div class="content has-text-justified" style="font-size:1rem;">
          </div>
        </div>
      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h3 class="title is-3" id="Metrics">Metrics Details</h3>
          <img src="static/images/Metrics.jpg" alt="Dataset Visualization" style="width:100%;display:block;margin-bottom:16px;"/>
          <div class="content has-text-justified" style="font-size:1rem;">
          </div>
        </div>
      </div>



      <button id="toggle-images" style="margin-bottom:12px;" class="button is-small is-link">Show/Hide Results Images</button>
      <div id="images-collapsible" style="display:none;">
        <div style="overflow-x: auto; white-space: nowrap; width: 100%; border-radius: 8px; border: 1px solid #eee; padding: 8px 0; margin-bottom: 24px;">
          <img src="static/images/result_tissue.jpg" alt="Dataset Visualization" style="height:320px; display:inline-block; margin-right:12px;"/>
          <img src="static/images/result_tissue_2.jpg" alt="Dataset Visualization" style="height:320px; display:inline-block; margin-right:12px;"/>
          <!-- 可以继续添加更多图片 -->
        </div>
        <div class="content has-text-justified" style="font-size:1rem;">
          We compared the point tracking datasets on tissue: SuPer, Sem.SuPer, SurgT, STIR, SugrgMotion (Tissue); the key point datasets of surgical instruments: EndoVis15, Du et al., SurgPose, SurgMotion (Instrument). We have counted the results of 5 different surgical scenarios. From the graphs and tables, we can see that Cauterization Smoke has the least impact on the tracking results. Instrument Occlusion, Camera Jitter and Surface Reflection have similar impacts, while Tissue Deformation has the greatest impact.
        </div>
      </div>
      <script>
        document.getElementById('toggle-images').onclick = function() {
          var box = document.getElementById('images-collapsible');
          box.style.display = box.style.display === 'none' ? 'block' : 'none';
        };
      </script>


      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h3 class="title is-3" id="ablation_study">Supplementary Ablation Study</h3>
          <img src="static/images/Ablation_Study.jpg" alt="Dataset Visualization" style="width:100%;display:block;margin-bottom:16px;"/>
          <div class="content has-text-justified" style="font-size:1rem;">
          </div>
        </div>
      </div>


      
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h3 class="title is-3">Visualization of point tracking (Tissue)</h3>
          <div class="container">
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item">
                <iframe width="100%" height="480" src="https://www.youtube.com/embed/0GQqcaQzdHs?autoplay=1&mute=1&loop=1&playlist=0GQqcaQzdHs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              </div>
            </div>
            <div style="font-size:0.95rem;color:#888;margin-top:8px;">
              We show the results of tissue point tracking using TG-SurgPT (Ours) (Blue), Track-On (Red), and MFT (Green) in five different scenarios. In particular, we selected some relatively long videos for visualization (about 6-20 seconds).
            </div>
          </div>
        </div>
      </div>

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h3 class="title is-3">Visualization of point tracking (Instrument)</h3>
          <div class="container">
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item">
                <iframe width="100%" height="480" src="https://www.youtube.com/embed/-OTjjJegdT0?autoplay=1&mute=1&loop=1&playlist=-OTjjJegdT0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
              </div>
            </div>
            <div style="font-size:0.95rem;color:#888;margin-top:8px;">
              We show the results of instrument point tracking using TG-SurgPT (Ours) (Blue), Track-On (Red), and MFT (Green) in five different scenarios.
            </div>
          </div>
        </div>
      </div>


      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h3 class="title is-3">Licensing</h3>
          <p>
            The original dataset and annotations of VL-SurgTPT are released under the Creative Commons Attribution (BY) license and cannot be used for commercial purposes. We provide a sample of the dataset in the <a
                href="https://github.com/SzuPc/VL-SurgPT-Dataset">Github</a> section for reference. After the ethical review, we will make our entire dataset public, and a written application form is required to obtain the data.
        </div>
      </div>

      <!-- <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h3 class="title is-3">Related Links</h3>
          <div class="content has-text-justified">
            <p>
              <a href="https://particle-video-revisited.github.io/">Particle Video Revisited: Tracking Through
                Occlusions Using Point Trajectories</a> propose Persistent Independent Particles (PIPs), a new particle
              video method that tracks any pixel over time.
            </p>
            <p>
              <a href="https://github.com/google-research/kubric">Kubric: A scalable dataset generator</a> is a data generation pipeline for creating semi-realistic synthetic multi-object videos with rich annotations such as instance segmentation masks, depth maps, and optical flow.
            </p>
          </div>
        </div>
      </div> -->
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://arxiv.org/abs/2211.03726">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/deepmind/tapnet" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0
                International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/tapvid/tapvid.github.io">source
                code</a> of this website, which itelf is a fork of <a
                href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. We just ask that you link back to this
              page in the footer.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
